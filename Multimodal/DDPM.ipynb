{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "\n",
    "# 함수에 입력된 값을 넣은 채로 새로운 함수를 정의할떄 사용\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision import transforms as T, utils\n",
    "\n",
    "# tensor의 차원을 차원 배열의 입력으로 편리하게 바꿀 수 있는 라이브러리\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from denoising_diffusion_pytorch.attend import Attend\n",
    "from denoising_diffusion_pytorch.fid_evaluation import FIDEvaluation\n",
    "\n",
    "from denoising_diffusion_pytorch.version import __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from packaging import version\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "# namedtuple을 사용하여 AttentionConfig 구조체 정의. \n",
    "# 이 구조체는 주목(Attention) 연산에 필요한 설정을 담고 있습니다. \n",
    "# 사용 예: AttentionConfig(True, True, True)\n",
    "AttentionConfig = namedtuple('AttentionConfig', ['enable_flash', 'enable_math', 'enable_mem_efficient'])\n",
    "\n",
    "# 주어진 값(val)이 None인지 여부를 확인하는 헬퍼 함수.\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# 주어진 값(val)이 None이 아니면 그 값을, 아니면 기본값(d)을 반환하는 헬퍼 함수.\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# 함수를 한 번만 호출할 수 있게 하는 데코레이터 정의.\n",
    "def once(fn):\n",
    "    called = False  # 해당 함수의 호출 여부를 추적하는 변수.\n",
    "    @wraps(fn)  # 원본 함수의 정보를 유지하기 위한 데코레이터.\n",
    "    def inner(x):\n",
    "        nonlocal called  # 외부 함수의 called 변수를 참조하기 위해 nonlocal 선언.\n",
    "        if called:  # 이미 호출되었다면, 더 이상 실행하지 않음.\n",
    "            return\n",
    "        called = True\n",
    "        return fn(x)  # 함수 최초 호출 시 실행 및 결과 반환.\n",
    "    return inner\n",
    "\n",
    "print_once = once(print)  # print 함수를 한 번만 호출 가능하게 설정.\n",
    "\n",
    "# Attend 클래스 정의: nn.Module을 상속받아 주목 메커니즘 구현.\n",
    "class Attend(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = 0.,  # 드롭아웃 확률.\n",
    "        flash = False,  # Flash 주목 사용 여부.\n",
    "        scale = None  # 스케일링 팩터, 기본값은 None.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.attn_dropout = nn.Dropout(dropout)  # 드롭아웃 층 초기화.\n",
    "\n",
    "        self.flash = flash  # Flash 주목 사용 설정.\n",
    "        # Flash 주목을 사용하기 위해선 PyTorch 2.0 이상이 필요함을 확인.\n",
    "        assert not (flash and version.parse(torch.__version__) < version.parse('2.0.0')), 'For flash attention, PyTorch 2.0 or above is required'\n",
    "\n",
    "        self.cpu_config = AttentionConfig(True, True, True)  # CPU용 설정.\n",
    "        self.cuda_config = None  # CUDA용 설정 초기화.\n",
    "\n",
    "        if not torch.cuda.is_available() or not flash:\n",
    "            return  # CUDA 사용 불가 또는 Flash 주목 비활성화 시 초기화 중단.\n",
    "\n",
    "        device_properties = torch.cuda.get_device_properties(torch.device('cuda'))  # 현재 CUDA 장치 속성 조회.\n",
    "\n",
    "        # A100 GPU 감지 시 Flash 주목 사용, 그렇지 않으면 다른 설정 사용.\n",
    "        if device_properties.major == 8 and device_properties.minor == 0:\n",
    "            print_once('A100 GPU detected, using flash attention if input tensor is on cuda')\n",
    "            self.cuda_config = AttentionConfig(True, False, False)\n",
    "        else:\n",
    "            print_once('Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda')\n",
    "            self.cuda_config = AttentionConfig(False, True, True)\n",
    "\n",
    "    def flash_attn(self, q, k, v):\n",
    "        # q, k, v 텐서의 차원을 분해하여 변수에 할당. q_len과 k_len은 쿼리와 키의 시퀀스 길이.\n",
    "        _, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n",
    "\n",
    "        if exists(self.scale):\n",
    "            default_scale = q.shape[-1]  # q 텐서의 마지막 차원 크기를 기본 스케일로 설정.\n",
    "            q = q * (self.scale / default_scale)  # 스케일 조정.\n",
    "\n",
    "        q, k, v = map(lambda t: t.contiguous(), (q, k, v))  # 연속 메모리 할당을 위해 contiguous 호출.\n",
    "\n",
    "        config = self.cuda_config if is_cuda else self.cpu_config  # 사용할 설정 선택.\n",
    "\n",
    "        # Flash 주목 실행. PyTorch 2.0에서는 flash 주목 연산을 지원.\n",
    "        with torch.backends.cuda.sdp_kernel(**config._asdict()):\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p = self.dropout if self.training else 0.\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        q_len, k_len, device = q.shape[-2], k.shape[-2], q.device  # 입력 텐서의 차원과 장치 정보 추출.\n",
    "\n",
    "        if self.flash:  # Flash 주목 활성화 시 flash_attn 메소드 호출.\n",
    "            return self.flash_attn(q, k, v)\n",
    "\n",
    "        scale = default(self.scale, q.shape[-1] ** -0.5)  # 스케일링 팩터 설정.\n",
    "\n",
    "        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k) * scale  # 쿼리와 키의 유사도 계산.\n",
    "\n",
    "        attn = sim.softmax(dim=-1)  # 소프트맥스를 적용하여 주목 확률 계산.\n",
    "        attn = self.attn_dropout(attn)  # 드롭아웃 적용.\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)  # 주목 확률을 사용하여 밸류 텐서 집계.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from pytorch_fid.fid_score import calculate_frechet_distance\n",
    "from pytorch_fid.inception import InceptionV3\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class FIDEvaluation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        dl,\n",
    "        sampler,\n",
    "        channels=3,\n",
    "        accelerator=None,\n",
    "        stats_dir=\"./results\",\n",
    "        device=\"cuda\",\n",
    "        num_fid_samples=50000,\n",
    "        inception_block_idx=2048,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = num_fid_samples\n",
    "        self.device = device\n",
    "        self.channels = channels\n",
    "        self.dl = dl\n",
    "        self.sampler = sampler\n",
    "        self.stats_dir = stats_dir\n",
    "        self.print_fn = print if accelerator is None else accelerator.print\n",
    "        assert inception_block_idx in InceptionV3.BLOCK_INDEX_BY_DIM\n",
    "        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[inception_block_idx]\n",
    "        self.inception_v3 = InceptionV3([block_idx]).to(device)\n",
    "        self.dataset_stats_loaded = False\n",
    "\n",
    "    def calculate_inception_features(self, samples):\n",
    "        if self.channels == 1:\n",
    "            samples = repeat(samples, \"b 1 ... -> b c ...\", c=3)\n",
    "\n",
    "        self.inception_v3.eval()\n",
    "        features = self.inception_v3(samples)[0]\n",
    "\n",
    "        if features.size(2) != 1 or features.size(3) != 1:\n",
    "            features = adaptive_avg_pool2d(features, output_size=(1, 1))\n",
    "        features = rearrange(features, \"... 1 1 -> ...\")\n",
    "        return features\n",
    "\n",
    "    def load_or_precalc_dataset_stats(self):\n",
    "        path = os.path.join(self.stats_dir, \"dataset_stats\")\n",
    "        try:\n",
    "            ckpt = np.load(path + \".npz\")\n",
    "            self.m2, self.s2 = ckpt[\"m2\"], ckpt[\"s2\"]\n",
    "            self.print_fn(\"Dataset stats loaded from disk.\")\n",
    "            ckpt.close()\n",
    "        except OSError:\n",
    "            num_batches = int(math.ceil(self.n_samples / self.batch_size))\n",
    "            stacked_real_features = []\n",
    "            self.print_fn(\n",
    "                f\"Stacking Inception features for {self.n_samples} samples from the real dataset.\"\n",
    "            )\n",
    "            for _ in tqdm(range(num_batches)):\n",
    "                try:\n",
    "                    real_samples = next(self.dl)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                real_samples = real_samples.to(self.device)\n",
    "                real_features = self.calculate_inception_features(real_samples)\n",
    "                stacked_real_features.append(real_features)\n",
    "            stacked_real_features = (\n",
    "                torch.cat(stacked_real_features, dim=0).cpu().numpy()\n",
    "            )\n",
    "            m2 = np.mean(stacked_real_features, axis=0)\n",
    "            s2 = np.cov(stacked_real_features, rowvar=False)\n",
    "            np.savez_compressed(path, m2=m2, s2=s2)\n",
    "            self.print_fn(f\"Dataset stats cached to {path}.npz for future use.\")\n",
    "            self.m2, self.s2 = m2, s2\n",
    "        self.dataset_stats_loaded = True\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def fid_score(self):\n",
    "        if not self.dataset_stats_loaded:\n",
    "            self.load_or_precalc_dataset_stats()\n",
    "        self.sampler.eval()\n",
    "        batches = num_to_groups(self.n_samples, self.batch_size)\n",
    "        stacked_fake_features = []\n",
    "        self.print_fn(\n",
    "            f\"Stacking Inception features for {self.n_samples} generated samples.\"\n",
    "        )\n",
    "        for batch in tqdm(batches):\n",
    "            fake_samples = self.sampler.sample(batch_size=batch)\n",
    "            fake_features = self.calculate_inception_features(fake_samples)\n",
    "            stacked_fake_features.append(fake_features)\n",
    "        stacked_fake_features = torch.cat(stacked_fake_features, dim=0).cpu().numpy()\n",
    "        m1 = np.mean(stacked_fake_features, axis=0)\n",
    "        s1 = np.cov(stacked_fake_features, rowvar=False)\n",
    "\n",
    "        return calculate_frechet_distance(m1, s1, self.m2, self.s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# constants\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# helpers functions\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def cast_tuple(t, length = 1):\n",
    "    if isinstance(t, tuple):\n",
    "        return t\n",
    "    return ((t,) * length)\n",
    "\n",
    "def divisible_by(numer, denom):\n",
    "    return (numer % denom) == 0\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def has_int_squareroot(num):\n",
    "    return (math.sqrt(num) ** 2) == num\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "def convert_image_to_fn(img_type, image):\n",
    "    if image.mode != img_type:\n",
    "        return image.convert(img_type)\n",
    "    return image\n",
    "\n",
    "# normalization functions\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# small helper modules\n",
    "\n",
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n",
    "    )\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n",
    "\n",
    "# sinusoidal positional embeds\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert divisible_by(dim, 2)\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "# building block modules\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        num_mem_kv = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "        self.mem_kv = nn.Parameter(torch.randn(2, heads, dim_head, num_mem_kv))\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        mk, mv = map(lambda t: repeat(t, 'h c n -> b h c n', b = b), self.mem_kv)\n",
    "        k, v = map(partial(torch.cat, dim = -1), ((mk, k), (mv, v)))\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        num_mem_kv = 4,\n",
    "        flash = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.attend = Attend(flash = flash)\n",
    "\n",
    "        self.mem_kv = nn.Parameter(torch.randn(2, heads, num_mem_kv, dim_head))\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1) # (batch_size, num_heads, seq_length, depth)의 구조를 갖는다.\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h (x y) c', h = self.heads), qkv)\n",
    "\n",
    "        mk, mv = map(lambda t: repeat(t, 'h n d -> b h n d', b = b), self.mem_kv)\n",
    "        k, v = map(partial(torch.cat, dim = -2), ((mk, k), (mv, v)))\n",
    "\n",
    "        out = self.attend(q, k, v)\n",
    "\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# model\n",
    "\n",
    "# U-Net 아키텍처 기반 디퓨전 모델을 정의하는 클래스입니다.\n",
    "class Unet(nn.Module):\n",
    "    # 모델 초기화 함수\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,  # 모델의 기본 차원 수입니다.\n",
    "        init_dim = None,  # 입력 컨볼루션 레이어의 차원 수입니다. 기본값은 `dim`과 동일합니다.\n",
    "        out_dim = None,  # 출력 컨볼루션 레이어의 차원 수입니다. 기본값은 입력 채널 수와 동일합니다.\n",
    "        dim_mults = (1, 2, 4, 8),  # 각 다운샘플링/업샘플링 단계에서 차원의 배수입니다.\n",
    "        channels = 3,  # 입력 이미지의 채널 수입니다.\n",
    "        self_condition = False,  # 자기 조건부 생성을 위한 플래그입니다.\n",
    "        resnet_block_groups = 8,  # ResNet 블록 내의 그룹 정규화에 사용되는 그룹 수입니다.\n",
    "        learned_variance = False,  # 출력에서 분산을 학습할지 여부를 결정합니다.\n",
    "        learned_sinusoidal_cond = False,  # 학습 가능한 사인 곡선 포지셔널 인코딩을 사용할지 결정합니다.\n",
    "        random_fourier_features = False,  # 무작위 포리에 특징을 사용할지 결정합니다.\n",
    "        learned_sinusoidal_dim = 16,  # 학습 가능한 사인 곡선 포지셔널 인코딩의 차원 수입니다.\n",
    "        sinusoidal_pos_emb_theta = 10000,  # 사인 곡선 포지셔널 인코딩의 스케일링 인자입니다.\n",
    "        attn_dim_head = 32,  # 주목 메커니즘의 헤드 당 차원 수입니다.\n",
    "        attn_heads = 4,  # 주목 메커니즘의 헤드 수입니다.\n",
    "        full_attn = None,  # 내부적으로 전체 주목 메커니즘을 사용할 단계를 지정합니다. 기본값은 마지막 레이어에만 전체 주목을 사용합니다.\n",
    "        flash_attn = False  # 플래시 주목 메커니즘을 사용할지 여부를 결정합니다.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 입력 채널 수를 설정합니다. 자기 조건부 생성이 활성화되면 채널 수가 두 배가 됩니다.\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        # 입력 컨볼루션 레이어의 차원을 설정합니다.\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        # 각 단계에서의 차원을 계산합니다.\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # 시간 임베딩에 사용되는 차원을 계산합니다.\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        # 시간 임베딩을 위한 레이어를 설정합니다.\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # 각 다운샘플링과 업샘플링 단계에서 사용될 레이어들을 초기화합니다.\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        # 중간 단계에서 사용될 블록과 주목 메커니즘을 초기화합니다.\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "        self.mid_attn = FullAttention(mid_dim, heads = attn_heads[-1], dim_head = attn_dim_head[-1])\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "\n",
    "        # 출력 컨볼루션 레이어를 설정합니다.\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    # 모델의 다운샘플링 요소 수를 기반으로 입력 이미지의 차원이 나누어떨어지는지 확인하는 속성입니다.\n",
    "    @property\n",
    "    def downsample_factor(self):\n",
    "        return 2 ** (len(self.downs) - 1)\n",
    "\n",
    "    # 모델의 순방향 패스를 정의합니다.\n",
    "    def forward(self, x, time, x_self_cond = None):\n",
    "        # 입력 이미지의 차원이 모델의 다운샘플링 요소로 나누어떨어지는지 확인합니다.\n",
    "        assert all([divisible_by(d, self.downsample_factor) for d in x.shape[-2:]]), f'your input dimensions {x.shape[-2:]} need to be divisible by {self.downsample_factor}, given the unet'\n",
    "\n",
    "        # 자기 조건부 생성을 위해 입력 이미지에 조건부 이미지를 결합합니다.\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim = 1)\n",
    "\n",
    "        # 입력 이미지에 초기 컨볼루션 레이어를 적용합니다.\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()  # 원본 입력 이미지를 복제합니다.\n",
    "\n",
    "        # 시간 임베딩을 계산합니다.\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []  # 각 단계의 출력을 저장하는 리스트입니다.\n",
    "\n",
    "        # 다운샘플링 단계를 수행합니다.\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)  # 첫 번째 블록을 적용합니다.\n",
    "            h.append(x)  # 출력을 저장합니다.\n",
    "\n",
    "            x = block2(x, t)  # 두 번째 블록을 적용합니다.\n",
    "            x = attn(x) + x  # 주목 메커니즘을 적용하고 결과를 더합니다.\n",
    "            h.append(x)  # 출력을 저장합니다.\n",
    "\n",
    "            x = downsample(x)  # 다운샘플링을 적용합니다.\n",
    "\n",
    "        # 중간 단계를 수행합니다.\n",
    "        x = self.mid_block1(x, t)  # 첫 번째 중간 블록을 적용합니다.\n",
    "        x = self.mid_attn(x) + x  # 중간 주목 메커니즘을 적용하고 결과를 더합니다.\n",
    "        x = self.mid_block2(x, t)  # 두 번째 중간 블록을 적용합니다.\n",
    "\n",
    "        # 업샘플링 단계를 수행합니다.\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)  # 이전 단계의 출력과 현재 출력을 결합합니다.\n",
    "            x = block1(x, t)  # 첫 번째 블록을 적용합니다.\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)  # 이전 단계의 출력과 현재 출력을 결합합니다.\n",
    "            x = block2(x, t)  # 두 번째 블록을 적용합니다.\n",
    "            x = attn(x) + x  # 주목 메커니즘을 적용하고 결과를 더합니다.\n",
    "\n",
    "            x = upsample(x)  # 업샘플링을 적용합니다.\n",
    "\n",
    "        # 최종 출력을 생성합니다.\n",
    "        x = torch.cat((x, r), dim = 1)  # 원본 입력 이미지와 현재 출력을 결합합니다.\n",
    "        x = self.final_res_block(x, t)  # 최종 ResNet 블록을 적용합니다.\n",
    "        return self.final_conv(x)  # 최종 컨볼루션 레이어를 적용하여 출력 이미지를 생성합니다.\n",
    "\n",
    "\n",
    "# gaussian diffusion trainer class\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original ddpm paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):\n",
    "    \"\"\"\n",
    "    sigmoid schedule\n",
    "    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "    better for images > 64x64, when used during training\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "    v_start = torch.tensor(start / tau).sigmoid()\n",
    "    v_end = torch.tensor(end / tau).sigmoid()\n",
    "    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        image_size,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        objective = 'pred_v',\n",
    "        beta_schedule = 'sigmoid',\n",
    "        schedule_fn_kwargs = dict(),\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = True,\n",
    "        offset_noise_strength = 0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "        min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556\n",
    "        min_snr_gamma = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n",
    "        assert not hasattr(model, 'random_or_learned_sinusoidal_cond') or not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            beta_schedule_fn = linear_beta_schedule\n",
    "        elif beta_schedule == 'cosine':\n",
    "            beta_schedule_fn = cosine_beta_schedule\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            beta_schedule_fn = sigmoid_beta_schedule\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.) #pad(input, pad, mode='constant', value=0)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # offset noise strength - in blogpost, they claimed 0.1 was ideal\n",
    "\n",
    "        self.offset_noise_strength = offset_noise_strength\n",
    "\n",
    "        # derive loss weight\n",
    "        # snr - signal noise ratio\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "        # https://arxiv.org/abs/2303.09556\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / snr)\n",
    "        elif objective == 'pred_x0':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr)\n",
    "        elif objective == 'pred_v':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / (snr + 1))\n",
    "\n",
    "        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.betas.device\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n",
    "        model_output = self.model(x, t, x_self_cond)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample(self, x, t: int, x_self_cond = None):\n",
    "        b, *_, device = *x.shape, self.device\n",
    "        batched_times = torch.full((b,), t, device = device, dtype = torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps = False):\n",
    "        batch, device = shape[0], self.device\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond)\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def ddim_sample(self, shape, return_all_timesteps = False):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                imgs.append(img)\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size = 16, return_all_timesteps = False):\n",
    "        image_size, channels = self.image_size, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):\n",
    "        b, c, h, w = x_start.shape\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # offset noise - https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "\n",
    "        offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength)\n",
    "\n",
    "        if offset_noise_strength > 0.:\n",
    "            offset_noise = torch.randn(x_start.shape[:2], device = self.device)\n",
    "            noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model(x, t, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
    "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)\n",
    "\n",
    "# dataset classes\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        image_size,\n",
    "        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n",
    "        augment_horizontal_flip = False,\n",
    "        convert_image_to = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "\n",
    "        maybe_convert_fn = partial(convert_image_to_fn, convert_image_to) if exists(convert_image_to) else nn.Identity()\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Lambda(maybe_convert_fn),\n",
    "            T.Resize(image_size),\n",
    "            T.RandomHorizontalFlip() if augment_horizontal_flip else nn.Identity(),\n",
    "            T.CenterCrop(image_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)\n",
    "\n",
    "# trainer class\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        folder,\n",
    "        *,\n",
    "        train_batch_size = 16,\n",
    "        gradient_accumulate_every = 1,\n",
    "        augment_horizontal_flip = True,\n",
    "        train_lr = 1e-4,\n",
    "        train_num_steps = 100000,\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        save_and_sample_every = 1000,\n",
    "        num_samples = 25,\n",
    "        results_folder = './results',\n",
    "        amp = False,\n",
    "        mixed_precision_type = 'fp16',\n",
    "        split_batches = True,\n",
    "        convert_image_to = None,\n",
    "        calculate_fid = True,\n",
    "        inception_block_idx = 2048,\n",
    "        max_grad_norm = 1.,\n",
    "        num_fid_samples = 50000,\n",
    "        save_best_and_latest_only = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # accelerator\n",
    "\n",
    "        self.accelerator = Accelerator(\n",
    "            split_batches = split_batches,\n",
    "            mixed_precision = mixed_precision_type if amp else 'no'\n",
    "        )\n",
    "\n",
    "        # model\n",
    "\n",
    "        self.model = diffusion_model\n",
    "        self.channels = diffusion_model.channels\n",
    "        is_ddim_sampling = diffusion_model.is_ddim_sampling\n",
    "\n",
    "        # default convert_image_to depending on channels\n",
    "\n",
    "        if not exists(convert_image_to):\n",
    "            convert_image_to = {1: 'L', 3: 'RGB', 4: 'RGBA'}.get(self.channels)\n",
    "\n",
    "        # sampling and training hyperparameters\n",
    "\n",
    "        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n",
    "        self.num_samples = num_samples\n",
    "        self.save_and_sample_every = save_and_sample_every\n",
    "\n",
    "        self.batch_size = train_batch_size\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "        assert (train_batch_size * gradient_accumulate_every) >= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n",
    "\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.image_size = diffusion_model.image_size\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        # dataset and dataloader\n",
    "\n",
    "        self.ds = Dataset(folder, self.image_size, augment_horizontal_flip = augment_horizontal_flip, convert_image_to = convert_image_to)\n",
    "\n",
    "        assert len(self.ds) >= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'\n",
    "\n",
    "        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n",
    "\n",
    "        dl = self.accelerator.prepare(dl)\n",
    "        self.dl = cycle(dl)\n",
    "\n",
    "        # optimizer\n",
    "\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
    "\n",
    "        # for logging results in a folder periodically\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n",
    "            self.ema.to(self.device)\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok = True)\n",
    "\n",
    "        # step counter state\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        # prepare model, dataloader, optimizer with accelerator\n",
    "\n",
    "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
    "\n",
    "        # FID-score computation\n",
    "\n",
    "        self.calculate_fid = calculate_fid and self.accelerator.is_main_process\n",
    "\n",
    "        if self.calculate_fid:\n",
    "            if not is_ddim_sampling:\n",
    "                self.accelerator.print(\n",
    "                    \"WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming.\"\\\n",
    "                    \"Consider using DDIM sampling to save time.\"\n",
    "                )\n",
    "            self.fid_scorer = FIDEvaluation(\n",
    "                batch_size=self.batch_size,\n",
    "                dl=self.dl,\n",
    "                sampler=self.ema.ema_model,\n",
    "                channels=self.channels,\n",
    "                accelerator=self.accelerator,\n",
    "                stats_dir=results_folder,\n",
    "                device=self.device,\n",
    "                num_fid_samples=num_fid_samples,\n",
    "                inception_block_idx=inception_block_idx\n",
    "            )\n",
    "\n",
    "        if save_best_and_latest_only:\n",
    "            assert calculate_fid, \"`calculate_fid` must be True to provide a means for model evaluation for `save_best_and_latest_only`.\"\n",
    "            self.best_fid = 1e10 # infinite\n",
    "\n",
    "        self.save_best_and_latest_only = save_best_and_latest_only\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    def save(self, milestone):\n",
    "        if not self.accelerator.is_local_main_process:\n",
    "            return\n",
    "\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,\n",
    "            'version': __version__\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n",
    "\n",
    "    def load(self, milestone):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device)\n",
    "\n",
    "        model = self.accelerator.unwrap_model(self.model)\n",
    "        model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema.load_state_dict(data[\"ema\"])\n",
    "\n",
    "        if 'version' in data:\n",
    "            print(f\"loading from version {data['version']}\")\n",
    "\n",
    "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
    "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
    "\n",
    "    def train(self):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
    "\n",
    "            while self.step < self.train_num_steps:\n",
    "\n",
    "                total_loss = 0.\n",
    "\n",
    "                for _ in range(self.gradient_accumulate_every):\n",
    "                    data = next(self.dl).to(device)\n",
    "\n",
    "                    with self.accelerator.autocast():\n",
    "                        loss = self.model(data)\n",
    "                        loss = loss / self.gradient_accumulate_every\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                    self.accelerator.backward(loss)\n",
    "\n",
    "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "\n",
    "                self.step += 1\n",
    "                if accelerator.is_main_process:\n",
    "                    self.ema.update()\n",
    "\n",
    "                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):\n",
    "                        self.ema.ema_model.eval()\n",
    "\n",
    "                        with torch.inference_mode():\n",
    "                            milestone = self.step // self.save_and_sample_every\n",
    "                            batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n",
    "\n",
    "                        all_images = torch.cat(all_images_list, dim = 0)\n",
    "\n",
    "                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n",
    "\n",
    "                        # whether to calculate fid\n",
    "\n",
    "                        if self.calculate_fid:\n",
    "                            fid_score = self.fid_scorer.fid_score()\n",
    "                            accelerator.print(f'fid_score: {fid_score}')\n",
    "                        if self.save_best_and_latest_only:\n",
    "                            if self.best_fid > fid_score:\n",
    "                                self.best_fid = fid_score\n",
    "                                self.save(\"best\")\n",
    "                            self.save(\"latest\")\n",
    "                        else:\n",
    "                            self.save(milestone)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        accelerator.print('training complete')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
